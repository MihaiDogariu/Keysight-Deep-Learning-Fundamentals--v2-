{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVEwAsnc4oYpO1430bmpwb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MihaiDogariu/Keysight-Deep-Learning-Fundamentals--v2-/blob/main/scripts/Unit_8_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression\n",
        "\n",
        "This notebook addresses the problem of regression: having a set of explanatory variables, we are interested to see if we can connect a dependent variable to them. In other words we want to predict one of the object's attributes based on the values of the rest of its attributes.\n",
        "\n",
        "This application aims to predict the fuel consumption of a set of cars, based on several other indicators."
      ],
      "metadata": {
        "id": "353ltSIuFdia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Make NumPy printouts easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "metadata": {
        "id": "TzGqgYTJWAcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and inspect the dataset"
      ],
      "metadata": {
        "id": "RvLg5X_UGcmr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhLAUkXYVjlV"
      },
      "outputs": [],
      "source": [
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "\n",
        "raw_dataset = pd.read_csv(url, names=column_names, na_values='?', comment='\\t',\n",
        "                          sep=' ', skipinitialspace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset"
      ],
      "metadata": {
        "id": "sujIOV2uW5gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auxiliary function to help with transforming from 'miles per gallon' to 'litres per 100 km'."
      ],
      "metadata": {
        "id": "Pe6V1KGJGiPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mpg_to_lper100km(mpg):\n",
        "  gallon = 3.785411784 # 1 gallon = 3.785411784 litres\n",
        "  mile = 1.609344 # 1 mile = 1.609344 km\n",
        "  lper100km = 100 * (gallon / mile) / mpg\n",
        "  return lper100km"
      ],
      "metadata": {
        "id": "ZqKoOPhCDtt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the above transformation on the entire dataset."
      ],
      "metadata": {
        "id": "4D9mp_j5Guol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = raw_dataset.rename(columns={\"MPG\": \"Consumption\"})\n",
        "dataset[\"Consumption\"] = dataset[\"Consumption\"].apply(lambda x: mpg_to_lper100km(x))\n",
        "dataset"
      ],
      "metadata": {
        "id": "Wg1RLr9gCuQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the data is correlated throughout the dataset."
      ],
      "metadata": {
        "id": "B_wA-YHnHdWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(dataset.corr(),annot=True,annot_kws={\"size\":10})"
      ],
      "metadata": {
        "id": "7Hsk2iKQNGuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it happens most of the time, the dataset might contain NULL or NaN values. We must find out if that also happens now."
      ],
      "metadata": {
        "id": "8QGy6LeGHi4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.isna().sum()"
      ],
      "metadata": {
        "id": "URFrph0f9Oc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We drop the NaN values since they represent only a tiny portion of the dataset. We plot the correlation matrix again, after dropping the features that do not hold a high enough correlation value with our w."
      ],
      "metadata": {
        "id": "pE4zeKamHxer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.dropna()"
      ],
      "metadata": {
        "id": "2812grEu931u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop(columns=[\"Acceleration\", \"Model Year\", \"Origin\"])\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(dataset.corr(),annot=True,annot_kws={\"size\":12})"
      ],
      "metadata": {
        "id": "BvhgjAtBCA5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot some more info about the dataset."
      ],
      "metadata": {
        "id": "Ye8TnUs8IJDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.describe()"
      ],
      "metadata": {
        "id": "_V89eWsw7HtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(dataset['Consumption'],histtype='bar')"
      ],
      "metadata": {
        "id": "HKi2lXiy9z7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are quite a few outliers for the 'Horsepower' attribute"
      ],
      "metadata": {
        "id": "7xLtmaZNIXN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.boxplot(column=\"Horsepower\")"
      ],
      "metadata": {
        "id": "aXP3nOnQA5QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auxiliary function to detect outliers and help reduce the dataset only to the 25-75 quartile range."
      ],
      "metadata": {
        "id": "3zZMAbmKIlTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_outliers(df,n,features):\n",
        "    \"\"\"\n",
        "    Takes a dataframe df of features and returns a list of the indices\n",
        "    corresponding to the observations containing more than n outliers according\n",
        "    to the Tukey method.\n",
        "    \"\"\"\n",
        "    outlier_indices = []\n",
        "\n",
        "    # iterate over features(columns)\n",
        "    for col in features:\n",
        "        # 1st quartile (25%)\n",
        "        # Q1 = np.percentile(df[col], 25)\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        # 3rd quartile (75%)\n",
        "        # Q3 = np.percentile(df[col],75)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        # Interquartile range (IQR)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        # outlier step\n",
        "        outlier_step = 1.5 * IQR\n",
        "\n",
        "        # Determine a list of indices of outliers for feature col\n",
        "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
        "\n",
        "        # append the found outlier indices for col to the list of outlier indices\n",
        "        outlier_indices.extend(outlier_list_col)\n",
        "\n",
        "    # select observations containing more than 2 outliers\n",
        "    outlier_indices = Counter(outlier_indices)\n",
        "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
        "\n",
        "    return multiple_outliers"
      ],
      "metadata": {
        "id": "rk72WgQXA5Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop = detect_outliers(dataset,0,['Consumption', 'Cylinders','Displacement','Horsepower','Weight'])\n",
        "dataset = dataset.drop(drop, axis = 0).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "berIb-YjA5Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop"
      ],
      "metadata": {
        "id": "dbzq088UA5JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.boxplot(column=\"Horsepower\")"
      ],
      "metadata": {
        "id": "45HXme_gA5Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dataset = dataset.copy()\n",
        "log_dataset[\"Consumption\"] = dataset[\"Consumption\"].apply(lambda x: np.log(1 + 100*x))\n",
        "\n",
        "f, axes = plt.subplots(1, 2)\n",
        "\n",
        "sns.kdeplot(dataset[[\"Consumption\"]], ax=axes[0], fill=True)\n",
        "sns.kdeplot(log_dataset[[\"Consumption\"]], ax=axes[1], fill=True)"
      ],
      "metadata": {
        "id": "sk8GUk6xR0EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data seems to be skewed, we can use a transformation that will help straighten it up, such as the log transform."
      ],
      "metadata": {
        "id": "GDsMvfTnIzVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_log_transform = True\n",
        "\n",
        "if use_log_transform:\n",
        "  dataset = log_dataset"
      ],
      "metadata": {
        "id": "Gfitwu1hSXpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we are finally done with the data pre-processing, we can move on and train our network. First, we create the train-test subsets."
      ],
      "metadata": {
        "id": "fdK0zRP_I8rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
        "train_features = train_dataset.drop(columns=[\"Consumption\"])\n",
        "train_labels = train_dataset[\"Consumption\"]\n",
        "\n",
        "test_dataset = dataset.drop(train_dataset.index)\n",
        "test_features = test_dataset.drop(columns=[\"Consumption\"])\n",
        "test_labels = test_dataset[\"Consumption\"]"
      ],
      "metadata": {
        "id": "HHZ7esLXBeW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the model architecture using the `torch.nn.Sequential()` approach."
      ],
      "metadata": {
        "id": "4COzMFdAJR3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define network dimensions\n",
        "n_input_dim = train_features.shape[1]\n",
        "# Layer size\n",
        "n_hidden = 10 # Number of hidden nodes\n",
        "n_output = 1 # Number of output nodes for predicted consumption\n",
        "\n",
        "# Build mdel\n",
        "torch_model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(n_input_dim, n_hidden),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(n_hidden, n_output),\n",
        ")\n",
        "\n",
        "print(torch_model)"
      ],
      "metadata": {
        "id": "XDE0Hsd2YmeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the loss, learning rate and optimizer."
      ],
      "metadata": {
        "id": "ZAMkmA4VJiz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = torch.nn.MSELoss() #Choosing mean square error as loss metric\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(torch_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "VG3MPA-uZb4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing the actual training."
      ],
      "metadata": {
        "id": "RQ3jO3hvJnnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_error = []\n",
        "test_error = []\n",
        "iters = 2000\n",
        "\n",
        "y_train_t = torch.FloatTensor(train_labels.values).reshape(-1,1) #Converting numpy array to torch tensor\n",
        "x_train_t = torch.nn.functional.normalize(torch.FloatTensor(train_features.values), dim=0)\n",
        "\n",
        "y_test_t = torch.FloatTensor(test_labels.values).reshape(-1,1) #Converting numpy array to torch tensor\n",
        "x_test_t = torch.nn.functional.normalize(torch.FloatTensor(test_features.values), dim=0)\n",
        "\n",
        "for i in range(iters):\n",
        "    torch_model.train(True)\n",
        "    # x_train_t = torch.FloatTensor(train_features.values)  #Converting numpy array to torch tensor\n",
        "    y_train_hat = torch_model(x_train_t)\n",
        "    # print(torch.transpose(y_hat, 0, 1))\n",
        "    loss = loss_func(y_train_hat, y_train_t)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    train_error.append(loss.item())\n",
        "\n",
        "    torch_model.train(False)\n",
        "    with torch.no_grad():\n",
        "      y_test_hat = torch_model(x_test_t)\n",
        "      test_loss = loss_func(y_test_hat, y_test_t)\n",
        "      test_error.append(test_loss.item())\n",
        "\n",
        "\n",
        "x = np.linspace(start=1, stop=iters, num=iters)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
        "ax.plot(x, train_error, color=\"red\", label=\"Train Loss\")\n",
        "ax.plot(x, test_error, color=\"blue\", label=\"Test Loss\")\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_title('Loss evolution')\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "2XLHKg5-ZdJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L5V2F4CCCQIb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}