{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<a href=\"https://colab.research.google.com/github/MihaiDogariu/Keysight-Deep-Learning-Fundamentals--v2-/blob/main/scripts/Unit_7_Fundamental_elements_of_a_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fundamental elements of a neural network\n",
    "This notebook approaches several aspects of neural networks. This particular application aims to forecast wether it will rain or not in the following day, based on the current day's observation. Therefore, this problem can be seen as a classification task."
   ],
   "metadata": {
    "id": "M0GdQZn0KHtI"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SpAc50W-5jqS",
    "ExecuteTime": {
     "end_time": "2025-03-24T23:26:06.356295Z",
     "start_time": "2025-03-24T23:26:03.318435Z"
    }
   },
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_SEED = 1 # for reproducibility\n",
    "torch.manual_seed(RANDOM_SEED)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c1ac8647b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Choosing the data"
   ],
   "metadata": {
    "id": "HkVt2mtcZ963"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the [\"Rain in Australia\"](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package?resource=download) dataset:"
   ],
   "metadata": {
    "id": "7fyidLy7NBS0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!gdown 1D-Ua952YzK95yPCJxzr8SWu7ZJGVROJd"
   ],
   "metadata": {
    "id": "LSiZJEkgM3vI",
    "ExecuteTime": {
     "end_time": "2025-03-24T23:26:51.444744Z",
     "start_time": "2025-03-24T23:26:44.454889Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1D-Ua952YzK95yPCJxzr8SWu7ZJGVROJd\n",
      "To: D:\\Keysight\\Programs\\weatherAUS.csv\n",
      "\n",
      "  0%|          | 0.00/14.1M [00:00<?, ?B/s]\n",
      "  7%|7         | 1.05M/14.1M [00:00<00:01, 8.28MB/s]\n",
      " 30%|##9       | 4.19M/14.1M [00:00<00:00, 20.1MB/s]\n",
      " 48%|####8     | 6.82M/14.1M [00:00<00:00, 18.3MB/s]\n",
      " 78%|#######8  | 11.0M/14.1M [00:00<00:00, 24.9MB/s]\n",
      "100%|##########| 14.1M/14.1M [00:00<00:00, 26.5MB/s]\n",
      "100%|##########| 14.1M/14.1M [00:00<00:00, 23.3MB/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each dataset entry contains 23 fields <-> 23 descriptors/features. We can have a look at its content by calling `pd.DataFrame.head()` on it."
   ],
   "metadata": {
    "id": "J1MGjpHrRjPS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('weatherAUS.csv')\n",
    "print(df.shape)\n",
    "print(list(df.columns))\n",
    "df.head(5)"
   ],
   "metadata": {
    "id": "yVhUHfqIRCLA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##2. Data pre-processing"
   ],
   "metadata": {
    "id": "DGZT9RZqaVmI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# The dataset contains a large set of attributes, but we are not interested in all of them\n",
    "# Let us make a list with the interesting attributes only\n",
    "keep = ['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity3pm', 'Pressure9am', 'RainToday', 'RainTomorrow']\n",
    "\n",
    "# We keep only the attributes that belong to the above list\n",
    "df_keep = df[keep]\n",
    "\n",
    "# Replace literal strings such as yes/no with numerical values 1/0\n",
    "df_keep['RainToday'].replace({'No': 0, 'Yes': 1}, inplace = True)\n",
    "df_keep['RainTomorrow'].replace({'No': 0, 'Yes': 1}, inplace = True)\n",
    "\n",
    "# The dataset also contains NaN values, which must be dealt with. Simplest solution is to eliminate them, altogether.\n",
    "df_keep = df_keep.dropna(how='any')\n",
    "df_keep.head(5)"
   ],
   "metadata": {
    "id": "QmmkMfi9Ry7x"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see the number of rainy and not rainy days"
   ],
   "metadata": {
    "id": "WotMmiqX3lOS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"No. of days when it did not rain: \", df_keep['RainTomorrow'].value_counts()[0])\n",
    "print(\"No. of days when it rained: \", df_keep['RainTomorrow'].value_counts()[1])\n",
    "\n",
    "# Display these values as a ratio of the entire dataset\n",
    "df_keep['RainTomorrow'].value_counts() / df_keep.shape[0]"
   ],
   "metadata": {
    "id": "gFkw5Asua3qI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    " We can observe that the dataset is not balanced, having 3.5 times more days when it did not rain as compared to the number of days when it rained."
   ],
   "metadata": {
    "id": "4rNlvHtYbdS4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# We can balance the dataset by under-sampling the over-represented subset\n",
    "under_sample = True\n",
    "\n",
    "if under_sample:\n",
    "  rain = df_keep[df_keep['RainTomorrow']==1] # select only the entries where rain was present\n",
    "  no_rain = df_keep[df_keep['RainTomorrow']==0] # select only the entries where rain was not present\n",
    "  no_rain = no_rain.sample(n=len(rain)) # pick len(rain) samples randomly from no_rain\n",
    "  df_keep = pd.concat([rain,no_rain],axis=0) # concatenate the 2 subsets and obtain a balanced dataset\n",
    "\n",
    "print(\"No. of days when it did not rain: \", df_keep['RainTomorrow'].value_counts()[0])\n",
    "print(\"No. of days when it rained: \", df_keep['RainTomorrow'].value_counts()[1])"
   ],
   "metadata": {
    "id": "bFpt29hqYQvo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The newly formed dataset will be now split into inputs and outputs. The outputs represent the labels associated with the input data. In this context, the labels are the decision rain/no rain."
   ],
   "metadata": {
    "id": "4iojMJGURQbk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = df_keep[keep[:-1]]\n",
    "y = df_keep[keep[-1]]"
   ],
   "metadata": {
    "id": "e8RaZ8MJhLcG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We split the dataset after the 70-15-15 train-val-test rule. The function `train_test_split` will split the data in only two subsets. Therefore, we must call it twice:\n",
    "1.   Divide the original dataset into `train_val` and `test`;\n",
    "2.   Divide `train_val` into `train` and `val`.\n",
    "\n"
   ],
   "metadata": {
    "id": "EuavNnT7ihHR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Divide the original dataset into 'train_val' and 'test':\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, test_size=test_ratio)\n",
    "print('train_val subset dimensions: {}\\ttest subset dimensions: {}'.format(x_train_val.shape, x_test.shape))\n",
    "\n",
    "# Divide 'train_val' into 'train' and 'val'\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=val_ratio/(val_ratio + train_ratio))\n",
    "print('train subset dimensions: {}\\tval subset dimensions: {}\\ttest subset dimensions:{}'.format(x_train.shape, x_val.shape, x_test.shape))"
   ],
   "metadata": {
    "id": "yp9gwFQfiZUz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We switch the data from `pandas.DataFrame` format to `Tensor` format in order for them to be processed by PyTorch."
   ],
   "metadata": {
    "id": "Zpl4yrYqTFJ0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x_train = torch.from_numpy(x_train.to_numpy()).float()\n",
    "y_train = torch.squeeze(torch.from_numpy(y_train.to_numpy()).float())\n",
    "\n",
    "x_val = torch.from_numpy(x_val.to_numpy()).float()\n",
    "y_val = torch.squeeze(torch.from_numpy(y_val.to_numpy()).float())\n",
    "\n",
    "x_test = torch.from_numpy(x_test.to_numpy()).float()\n",
    "y_test = torch.squeeze(torch.from_numpy(y_test.to_numpy()).float())\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ],
   "metadata": {
    "id": "k65jnrT5l-qj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##3. Training and validating the model"
   ],
   "metadata": {
    "id": "6wHvU5kumUEo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step is creating the neural network model. In PyTorch, models are usually created as a class that inherits the `torch.nn.Module` class. The model must define 2 functions:\n",
    "\n",
    "\n",
    "1.   `__init__()` - sets the general component structure of the network;\n",
    "2.   `forward()` - sets the behaviour for the feed-forward.\n",
    "\n"
   ],
   "metadata": {
    "id": "9AU-WGJ1XsaU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Net(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, n_features): # the n_features argument is used to establish the dimension of the input layer\n",
    "    super(Net, self).__init__()\n",
    "    self.fc1 = torch.nn.Linear(n_features, 5) # fully connected layer that connects the input to a 5-neuron layer\n",
    "    self.fc2 = torch.nn.Linear(5, 3) # fully connected layer that connects the previous layer to a 3-neuron layer\n",
    "    self.fc3 = torch.nn.Linear(3, 1) # fully connected layer that connects the previous layer to a 1-neuron layer - binary decision\n",
    "\n",
    "  def forward(self, x): # applying the activation function after running each fully connected layer\n",
    "    x = torch.nn.functional.relu(self.fc1(x))\n",
    "    x = torch.nn.functional.relu(self.fc2(x))\n",
    "    return torch.sigmoid(self.fc3(x)) # sigmoid for binary classification"
   ],
   "metadata": {
    "id": "sDpO8Pl8mTqg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initializing the network, the cost function and the optimizier"
   ],
   "metadata": {
    "id": "-lGYLICmZlxl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "net=Net(x_train.shape[1]) # it calls the Net.__init()__ function for model initialization\n",
    "criterion = torch.nn.BCELoss() # Binary Cross-Entropy Loss is the loss function\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01) # Adam optimizer"
   ],
   "metadata": {
    "id": "lQhWLMRDmqg3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading the functions and the data on GPU"
   ],
   "metadata": {
    "id": "521c_blhbCpO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "x_val = x_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = criterion.to(device)"
   ],
   "metadata": {
    "id": "frhlNoItnPhs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Auxiliary functions"
   ],
   "metadata": {
    "id": "pv_gU9U_bKJx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Compute accuracy\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "  predicted = y_pred.ge(.5).view(-1)\n",
    "  return (y_true == predicted).sum().float() / len(y_true)\n",
    "\n",
    "# Define a rounding function\n",
    "def round_tensor(t, decimal_places=3):\n",
    "  return round(t.item(), decimal_places)"
   ],
   "metadata": {
    "id": "q1iy47cbnZSO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model training"
   ],
   "metadata": {
    "id": "PE9K7L1Sb7jc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for epoch in range(1000):\n",
    "\n",
    "    y_pred = net(x_train) # forward propagation\n",
    "    y_pred = torch.squeeze(y_pred)\n",
    "    train_loss = criterion(y_pred, y_train) # compute cost function\n",
    "\n",
    "    if epoch % 100 == 0: # perform validation once in a while\n",
    "      train_acc = calculate_accuracy(y_train, y_pred) # compute train accuracy\n",
    "\n",
    "      y_val_pred = net(x_val) # forward propagation\n",
    "      y_pred = torch.squeeze(y_pred)\n",
    "      y_val_pred = torch.squeeze(y_val_pred)\n",
    "\n",
    "      val_loss = criterion(y_val_pred, y_val) # compute cost function\n",
    "\n",
    "      val_acc = calculate_accuracy(y_val, y_val_pred) # compute validation accuracy\n",
    "      print(\"epoch {}\\nTrain set - loss: {}, accuracy: {}\\nTest  set - loss: {}, accuracy: {}\"\n",
    "            .format(epoch,\n",
    "                    round_tensor(train_loss), round_tensor(train_acc),\n",
    "                    round_tensor(val_loss), round_tensor(val_acc)))\n",
    "\n",
    "    optimizer.zero_grad() # erase existing gradients\n",
    "\n",
    "    train_loss.backward() # compute gradients for current iteration\n",
    "\n",
    "    optimizer.step() # perform weights update"
   ],
   "metadata": {
    "id": "_nVIaqzNnvXP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "After training and optimization on `train` and `val`, we perform another test on the `test` subset."
   ],
   "metadata": {
    "id": "YxDbDq3UvDPC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "classes = ['No rain', 'Raining']\n",
    "\n",
    "y_pred = net(x_test)\n",
    "\n",
    "y_pred = y_pred.ge(.5).view(-1).cpu()\n",
    "y_test = y_test.cpu()\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=classes))"
   ],
   "metadata": {
    "id": "1-FeKlWMontP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "\n",
    "hmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label');"
   ],
   "metadata": {
    "id": "x3dNfXpmpOhU"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
