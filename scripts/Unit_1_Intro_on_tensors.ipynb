{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MihaiDogariu/Keysight-Deep-Learning-Fundamentals--v2-/blob/main/scripts/Unit_1_Intro_on_tensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to tensors\n",
        "\n",
        "\n",
        "This notebook makes a short introduction to working with *tensors*, the fundamental object of Deep Learning frameworks, such as PyTorch or Tensorflow.\n",
        "\n",
        "Generally speaking, tensors are N-dimensional arrays, and can be considered an extension of classical NumPy arrays. The core difference between the two, is that tensors are specifically designed to be run on GPUs, making tensor operations a few orders of magnitude faster than their CPU counter-parts."
      ],
      "metadata": {
        "id": "tK1Ji0a6ll27"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCOO5F3rdssJ",
        "ExecuteTime": {
          "end_time": "2025-03-26T23:38:42.667835Z",
          "start_time": "2025-03-26T23:38:37.806681Z"
        },
        "outputId": "34fc8815-5c9c-4d58-fec1-be1e31276c8c"
      },
      "source": [
        "!pip install ipython-autotime\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# We will use the autotime command to get the running time of each code block and investigate the difference\n",
        "%load_ext autotime"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipython-autotime in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (0.3.2)\n",
            "Requirement already satisfied: ipython in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from ipython-autotime) (8.34.0)\n",
            "Requirement already satisfied: colorama in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (0.4.6)\n",
            "Requirement already satisfied: decorator in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (5.2.1)\n",
            "Requirement already satisfied: exceptiongroup in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (1.2.2)\n",
            "Requirement already satisfied: jedi>=0.16 in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (0.1.7)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (3.0.50)\n",
            "Requirement already satisfied: pygments>=2.4.0 in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (2.19.1)\n",
            "Requirement already satisfied: stack_data in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (0.6.3)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (5.14.3)\n",
            "Requirement already satisfied: typing_extensions>=4.6 in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (4.12.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: wcwidth in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->ipython-autotime) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from stack_data->ipython->ipython-autotime) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from stack_data->ipython->ipython-autotime) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in d:\\work\\keysight\\2025\\programs\\.venv\\lib\\site-packages (from stack_data->ipython->ipython-autotime) (0.2.3)\n",
            "time: 0 ns (started: 2025-03-27 01:38:42 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, lets create two similar N-D arrays: an np array and a tensor, of the same dimensions. We will populate them with:\n",
        "- [`np.random.rand()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html)\n",
        "- [`torch.rand()`](https://pytorch.org/docs/stable/generated/torch.rand.html)\n",
        "\n",
        "By default, np arrays use `float64`, whereas tensors use `float32` data. We can convert the np arrays to a similar data type and re-run the same operations. We can use the `np.float32()` cast operator or the [`np.ndarray.astype()`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.astype.html) function.\n"
      ],
      "metadata": {
        "id": "oAQDyZ5gnEZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array_size = (3, 3)\n",
        "\n",
        "# 2D arrays\n",
        "array1 = np.random.rand(*array_size).astype(np.float32)\n",
        "array2 = np.random.rand(*array_size).astype(np.float32)\n",
        "\n",
        "# 2D tensors\n",
        "tensor1 = torch.rand(*array_size)\n",
        "tensor2 = torch.rand(*array_size)"
      ],
      "metadata": {
        "id": "5jrdeJwYNChF",
        "ExecuteTime": {
          "end_time": "2025-03-26T23:38:42.711611Z",
          "start_time": "2025-03-26T23:38:42.673818Z"
        },
        "outputId": "f4ae8825-332b-45d7-a186-07b6cd8626f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 15 ms (started: 2025-03-27 01:38:42 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print the data. We can also have a look at the type of data (`.dtype` attribute).\n",
        "\n",
        "Tensors, unlike np arrays, can be run on both CPU and GPU. They have an attribute, `.device` that holds this particular information. Let's print and see where the tensor resides upon default creation."
      ],
      "metadata": {
        "id": "5SKreyb_oSQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Np array values:')\n",
        "print(array1)\n",
        "print('\\nTensor values:')\n",
        "print(tensor1)\n",
        "\n",
        "print(f'\\nNp array data type is {array1.dtype}')\n",
        "print(f'Tensor data type is {tensor1.dtype}')\n",
        "print(f'Tensor is loaded on {tensor1.device}')"
      ],
      "metadata": {
        "id": "Uhyd-KpnStuL",
        "ExecuteTime": {
          "end_time": "2025-03-26T23:38:43.012722Z",
          "start_time": "2025-03-26T23:38:42.990587Z"
        },
        "outputId": "2166b0cb-bfa8-4044-8cbc-184257d85068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Np array values:\n",
            "[[0.09254928 0.19802298 0.6852597 ]\n",
            " [0.8152934  0.6896014  0.3033488 ]\n",
            " [0.38212073 0.5211278  0.59452564]]\n",
            "\n",
            "Tensor values:\n",
            "tensor([[0.2452, 0.4294, 0.7648],\n",
            "        [0.0185, 0.3601, 0.2743],\n",
            "        [0.1359, 0.5836, 0.6405]])\n",
            "\n",
            "Np array data type is float32\n",
            "Tensor data type is torch.float32\n",
            "Tensor is loaded on cpu\n",
            "time: 16 ms (started: 2025-03-27 01:38:42 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "TFN84Us31IfX"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's see how fast the operations are on both types of arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, run a multiplication operation on the np arrays with [`np.matmul()`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html#numpy-matmul):"
      ],
      "metadata": {
        "id": "7oiaC1tvv12X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_np = np.matmul(array1, array2)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wdU6bLhHOnwt",
        "ExecuteTime": {
          "end_time": "2025-03-26T23:38:43.059329Z",
          "start_time": "2025-03-26T23:38:43.044416Z"
        },
        "outputId": "1fb81b1e-8558-4a54-aafb-1dce49e05308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0 ns (started: 2025-03-27 01:38:43 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Y7lZwTe91IfY"
      },
      "cell_type": "markdown",
      "source": [
        "Then, a similar multiplication on the tensors with [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_tensor = torch.matmul(tensor1, tensor2)"
      ],
      "metadata": {
        "id": "gPauDniX0yTd",
        "ExecuteTime": {
          "end_time": "2025-03-26T23:38:43.126154Z",
          "start_time": "2025-03-26T23:38:43.105362Z"
        },
        "outputId": "c42b97af-44b8-43a8-f402-439daa3c42c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 15 ms (started: 2025-03-27 01:38:43 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The processing time is nearly identical in this case. So let's see the GPU speedup (if any). In order to do so, we must first check if our system has a dedicated GPU with CUDA support. We can check with `torch.cuda.is_available()`."
      ],
      "metadata": {
        "id": "fSaj92Qw0o-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "z44L854rlzgv",
        "ExecuteTime": {
          "end_time": "2025-03-26T23:38:44.857909Z",
          "start_time": "2025-03-26T23:38:43.134282Z"
        },
        "outputId": "8feaec46-7b6f-45f4-d355-24bb1744f952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "time: 1.72 s (started: 2025-03-27 01:38:43 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there is such a device available, then we can load the tensors on it, with the [`torch.Tensor.to()`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html) method."
      ],
      "metadata": {
        "id": "Oxk6FNx_24sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = tensor1.to(device)\n",
        "tensor2 = tensor2.to(device)"
      ],
      "metadata": {
        "id": "GsOe5tJmksJY",
        "ExecuteTime": {
          "end_time": "2025-03-26T23:38:44.990683Z",
          "start_time": "2025-03-26T23:38:44.881612Z"
        },
        "outputId": "3111de3c-84a4-489d-cbfe-2be5bfb8a548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 109 ms (started: 2025-03-27 01:38:44 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we re-run the multiplication, only that this time it will be done on the GPU, instead of the CPU."
      ],
      "metadata": {
        "id": "ZNfNNQn33Qcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_tensor = torch.matmul(tensor1, tensor2)"
      ],
      "metadata": {
        "id": "acT989b8oARa",
        "ExecuteTime": {
          "end_time": "2025-03-26T23:38:45.286542Z",
          "start_time": "2025-03-26T23:38:45.171040Z"
        },
        "outputId": "aa3adf51-f18f-40da-b759-184c12e08b36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 109 ms (started: 2025-03-27 01:38:45 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, the difference is not spectacular at all, but that is due to the very small size of the arrays. We have to pump those numbers up! Those are rookie numbers.\n",
        "\n",
        "So we will re-run the above operations, but on a larger scale. Switch `array_size` to 100 x 100 x 100 x 100 and see what the GPU does now."
      ],
      "metadata": {
        "id": "l6braDBk3iEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array_size = (100, 100, 100, 100)\n",
        "array1 = np.float32(np.random.rand(*array_size))\n",
        "array2 = np.float32(np.random.rand(*array_size))\n",
        "tensor1 = torch.rand(*array_size).to(device)\n",
        "tensor2 = torch.rand(*array_size).to(device)"
      ],
      "metadata": {
        "id": "GiSu3Gl2pcOh",
        "ExecuteTime": {
          "end_time": "2025-03-26T23:38:49.332805Z",
          "start_time": "2025-03-26T23:38:45.310701Z"
        },
        "outputId": "3a549651-df18-42fd-db10-6017d0db7663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 4 s (started: 2025-03-27 01:38:45 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "res_np = np.matmul(array1, array2)"
      ],
      "metadata": {
        "id": "sIm5FCbupd2R",
        "ExecuteTime": {
          "end_time": "2025-03-26T23:38:50.207755Z",
          "start_time": "2025-03-26T23:38:49.550371Z"
        },
        "outputId": "43740b51-45f1-44ee-8588-a41479a299b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 640 ms (started: 2025-03-27 01:38:49 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "res_tensor = torch.matmul(tensor1, tensor2)"
      ],
      "metadata": {
        "id": "I547_wTAqgwB",
        "ExecuteTime": {
          "end_time": "2025-03-26T23:38:50.449642Z",
          "start_time": "2025-03-26T23:38:50.410527Z"
        },
        "outputId": "eda5f524-6bbd-4279-a164-0e1e6e81de3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 16 ms (started: 2025-03-27 01:38:50 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to extract numeric values from the tensor and use them as Python numbers, we have several options:\n",
        "- index the tensor and then call [`torch.Tensor.item()`](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html) on a single value;\n",
        "- convert the tensor to a list [`torch.Tensor.tolist()`](https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html#torch.Tensor.tolist) and treat it as a Python list;\n",
        "- convert the tensor to a np array [`torch.Tensor.numpy()`]() and treat it as a np array - this step needs the tensor to be transfered back on CPU before-hand with: `torch.Tensor.cpu()`."
      ],
      "metadata": {
        "id": "BMtEj9lj-2M3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = res_tensor.cpu().numpy()\n",
        "print(x[0][0])"
      ],
      "metadata": {
        "id": "oVfJ4MdA-428",
        "ExecuteTime": {
          "end_time": "2025-03-26T23:38:50.972864Z",
          "start_time": "2025-03-26T23:38:50.851617Z"
        },
        "outputId": "b3b6386e-caf3-448a-dfad-b4b97eb26e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[21.486055 22.8759   25.30208  ... 22.386066 20.962614 24.18737 ]\n",
            " [25.731264 24.931953 27.061218 ... 25.293928 25.31982  28.41498 ]\n",
            " [26.403093 25.883512 24.406162 ... 24.500051 24.762085 27.099932]\n",
            " ...\n",
            " [25.51644  24.542372 28.432758 ... 26.357706 24.964693 27.648375]\n",
            " [24.225964 25.506601 25.464682 ... 26.73598  24.753082 27.741743]\n",
            " [30.166197 27.59787  27.955866 ... 30.11703  29.02287  29.481533]]\n",
            "time: 125 ms (started: 2025-03-27 01:38:50 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    }
  ]
}