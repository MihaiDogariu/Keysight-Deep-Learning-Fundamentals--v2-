{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MihaiDogariu/Keysight-Deep-Learning-Fundamentals--v2-/blob/main/scripts/Unit_1_Intro_on_tensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to tensors\n",
        "\n",
        "\n",
        "This notebook makes a short introduction to working with *tensors*, the fundamental object of Deep Learning frameworks, such as PyTorch or Tensorflow.\n",
        "\n",
        "Generally speaking, tensors are N-dimensional arrays, and can be considered an extension of classical NumPy arrays. The core difference between the two, is that tensors are specifically designed to be run on GPUs, making tensor operations a few orders of magnitude faster than their CPU counter-parts."
      ],
      "metadata": {
        "id": "tK1Ji0a6ll27"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCOO5F3rdssJ",
        "ExecuteTime": {
          "end_time": "2025-03-25T08:40:20.067818Z",
          "start_time": "2025-03-25T08:40:13.033321Z"
        },
        "outputId": "812e44a9-ac0d-4395-8b16-275bfa9675d9"
      },
      "source": [
        "!pip install ipython-autotime\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# We will use the autotime command to get the running time of each code block and investigate the difference\n",
        "%load_ext autotime"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipython-autotime in d:\\keysight\\programs\\.venv\\lib\\site-packages (0.3.2)\n",
            "Requirement already satisfied: ipython in d:\\keysight\\programs\\.venv\\lib\\site-packages (from ipython-autotime) (8.34.0)\n",
            "Requirement already satisfied: colorama in d:\\keysight\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (0.4.6)\n",
            "Requirement already satisfied: decorator in d:\\keysight\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (5.2.1)\n",
            "Requirement already satisfied: exceptiongroup in d:\\keysight\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (1.2.2)\n",
            "Requirement already satisfied: jedi>=0.16 in d:\\keysight\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in d:\\keysight\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (0.1.7)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\keysight\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (3.0.50)\n",
            "Requirement already satisfied: pygments>=2.4.0 in d:\\keysight\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (2.19.1)\n",
            "Requirement already satisfied: stack_data in d:\\keysight\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (0.6.3)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in d:\\keysight\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (5.14.3)\n",
            "Requirement already satisfied: typing_extensions>=4.6 in d:\\keysight\\programs\\.venv\\lib\\site-packages (from ipython->ipython-autotime) (4.12.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\keysight\\programs\\.venv\\lib\\site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: wcwidth in d:\\keysight\\programs\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->ipython-autotime) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in d:\\keysight\\programs\\.venv\\lib\\site-packages (from stack_data->ipython->ipython-autotime) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in d:\\keysight\\programs\\.venv\\lib\\site-packages (from stack_data->ipython->ipython-autotime) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in d:\\keysight\\programs\\.venv\\lib\\site-packages (from stack_data->ipython->ipython-autotime) (0.2.3)\n",
            "time: 0 ns (started: 2025-03-25 10:40:20 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, lets create two similar N-D arrays: an np array and a tensor, of the same dimensions. We will populate them with:\n",
        "- [`np.random.rand()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html)\n",
        "- [`torch.rand()`](https://pytorch.org/docs/stable/generated/torch.rand.html)\n"
      ],
      "metadata": {
        "id": "oAQDyZ5gnEZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array_size = (3, 3)\n",
        "\n",
        "# 2D arrays\n",
        "array1 = np.random.rand(*array_size)\n",
        "array2 = np.random.rand(*array_size)\n",
        "\n",
        "# 2D tensors\n",
        "tensor1 = torch.rand(*array_size)\n",
        "tensor2 = torch.rand(*array_size)"
      ],
      "metadata": {
        "id": "5jrdeJwYNChF",
        "ExecuteTime": {
          "end_time": "2025-03-25T08:52:22.727226Z",
          "start_time": "2025-03-25T08:52:22.579420Z"
        },
        "outputId": "6c48a635-74c3-430c-dd2a-e53c2bc7ae4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 125 ms (started: 2025-03-25 10:52:22 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print the data. We can also have a look at the type of data (`.dtype` attribute)"
      ],
      "metadata": {
        "id": "5SKreyb_oSQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Np array values:')\n",
        "print(array1)\n",
        "print('\\nTensor values:')\n",
        "print(tensor1)\n",
        "\n",
        "print(f'\\nNp array data type is {array1.dtype}')\n",
        "print(f'Tensor data type is {tensor1.dtype}')"
      ],
      "metadata": {
        "id": "Uhyd-KpnStuL",
        "ExecuteTime": {
          "end_time": "2025-03-25T08:52:34.128433Z",
          "start_time": "2025-03-25T08:52:34.104110Z"
        },
        "outputId": "73fd5c65-5824-4800-bba7-b852a21574b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Np array values:\n",
            "[[0.09549143 0.45628156 0.4967133 ]\n",
            " [0.85196747 0.74346317 0.78368142]\n",
            " [0.08719396 0.67572341 0.31404143]]\n",
            "\n",
            "Tensor values:\n",
            "tensor([[0.4142, 0.9587, 0.6295],\n",
            "        [0.3194, 0.4022, 0.2712],\n",
            "        [0.2967, 0.6455, 0.0691]])\n",
            "\n",
            "Np array data type is float64\n",
            "Tensor data type is torch.float32\n",
            "time: 15 ms (started: 2025-03-25 10:52:34 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors, unlike np arrays, can be run on both CPU and GPU. They have an attribute, `.device` that holds this particular information. Let's print and see where the tensor resides upon default creation."
      ],
      "metadata": {
        "id": "NG3VfDkouwlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor1.device)"
      ],
      "metadata": {
        "id": "NYxRfY2wrNwl",
        "ExecuteTime": {
          "end_time": "2025-03-25T08:53:41.837302Z",
          "start_time": "2025-03-25T08:53:41.821308Z"
        },
        "outputId": "4aa409e5-c336-4f07-94da-a6f7d0397a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "time: 0 ns (started: 2025-03-25 10:53:41 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how fast the operations are on both types of arrays."
      ],
      "metadata": {
        "id": "gBcrIuiQvgv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, run a multiplication operation on the np arrays with [`np.matmul()`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html#numpy-matmul):"
      ],
      "metadata": {
        "id": "7oiaC1tvv12X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_np = np.matmul(array1, array2)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wdU6bLhHOnwt",
        "ExecuteTime": {
          "end_time": "2025-03-25T08:54:32.059553Z",
          "start_time": "2025-03-25T08:54:32.051195Z"
        },
        "outputId": "aa356201-cae3-4779-df7f-903a9e1addd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0 ns (started: 2025-03-25 10:54:32 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, a similar multiplication on the tensors with [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul):"
      ],
      "metadata": {
        "id": "nS31kcjrwc34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_tensor = torch.matmul(tensor1, tensor2)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CoVeab8zOo-C",
        "ExecuteTime": {
          "end_time": "2025-03-25T08:55:01.769924Z",
          "start_time": "2025-03-25T08:55:01.751945Z"
        },
        "outputId": "53360f01-b4e5-4cbd-cfb3-a8b2aab362c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0 ns (started: 2025-03-25 10:55:01 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference in running time is mainly due to the different types of data contained by the np arrays and the tensors. By default, np arrays use `float64`, whereas tensors use `float32` data. We can convert the np arrays to a similar data type and re-run the same operations. We can use the `np.float32()` cast operator or the [`np.ndarray.astype()`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.astype.html) function."
      ],
      "metadata": {
        "id": "opkKjtfOxLG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array1_float32 = array1.astype(np.float32)\n",
        "array2_float32 = array2.astype(np.float32)"
      ],
      "metadata": {
        "id": "uAKX9WOJkzRH",
        "ExecuteTime": {
          "end_time": "2025-03-25T08:55:29.137674Z",
          "start_time": "2025-03-25T08:55:29.122122Z"
        },
        "outputId": "db55b31e-d43f-42cf-fed6-416b01b2de6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0 ns (started: 2025-03-25 10:55:29 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "res_np_float32 = np.matmul(array1_float32, array2_float32)"
      ],
      "metadata": {
        "id": "bc325qizlZ_Q",
        "ExecuteTime": {
          "end_time": "2025-03-25T08:55:41.795389Z",
          "start_time": "2025-03-25T08:55:41.779349Z"
        },
        "outputId": "a43c3f22-244b-4cac-8f5e-9fd98b1df540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0 ns (started: 2025-03-25 10:55:41 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "res_tensor = torch.matmul(tensor1, tensor2)"
      ],
      "metadata": {
        "id": "gPauDniX0yTd",
        "ExecuteTime": {
          "end_time": "2025-03-25T08:55:46.620672Z",
          "start_time": "2025-03-25T08:55:46.613819Z"
        },
        "outputId": "8b2c96b2-5c95-46b5-b8b3-f63698603932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0 ns (started: 2025-03-25 10:55:46 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the two operations run in a similar time span. So let's see the GPU speedup (if any). In order to do so, we must first check if our system has a dedicated GPU with CUDA support. We can check with `torch.cuda.is_available()`."
      ],
      "metadata": {
        "id": "fSaj92Qw0o-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "z44L854rlzgv",
        "ExecuteTime": {
          "end_time": "2025-03-25T08:56:56.278596Z",
          "start_time": "2025-03-25T08:56:55.288119Z"
        },
        "outputId": "093f7fa8-d4c8-4126-84e3-0414c2b2fd5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "time: 985 ms (started: 2025-03-25 10:56:55 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there is such a device available, then we can load the tensors on it, with the [`torch.Tensor.to()`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html) method."
      ],
      "metadata": {
        "id": "Oxk6FNx_24sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = tensor1.to(device)\n",
        "tensor2 = tensor2.to(device)"
      ],
      "metadata": {
        "id": "GsOe5tJmksJY",
        "ExecuteTime": {
          "end_time": "2025-03-25T08:57:27.767449Z",
          "start_time": "2025-03-25T08:57:27.436101Z"
        },
        "outputId": "e193cb93-f6bc-42d2-e4b6-140ad2baec3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 313 ms (started: 2025-03-25 10:57:27 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we re-run the multiplication, only that this time it will be done on the GPU, instead of the CPU."
      ],
      "metadata": {
        "id": "ZNfNNQn33Qcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_tensor = torch.matmul(tensor1, tensor2)"
      ],
      "metadata": {
        "id": "acT989b8oARa",
        "ExecuteTime": {
          "end_time": "2025-03-25T08:59:17.223Z",
          "start_time": "2025-03-25T08:59:17.215903Z"
        },
        "outputId": "8c94d4a1-bf0f-4d1f-e061-e872cf284325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0 ns (started: 2025-03-25 10:59:17 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, the difference is not spectacular at all, but that is due to the very small size of the arrays. We have to pump those numbers up! Those are rookie numbers.\n",
        "\n",
        "So we will re-run the above operations, but on a larger scale. Switch `array_size` to 100 x 100 x 100 x 100 and see what the GPU does now."
      ],
      "metadata": {
        "id": "l6braDBk3iEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array_size = (100, 100, 100, 100)\n",
        "array1 = np.float32(np.random.rand(*array_size))\n",
        "array2 = np.float32(np.random.rand(*array_size))\n",
        "tensor1 = torch.rand(*array_size).to(device)\n",
        "tensor2 = torch.rand(*array_size).to(device)"
      ],
      "metadata": {
        "id": "GiSu3Gl2pcOh",
        "ExecuteTime": {
          "end_time": "2025-03-25T08:59:54.248099Z",
          "start_time": "2025-03-25T08:59:50.100713Z"
        },
        "outputId": "8f87b9c3-208f-4f9e-c4ff-0eecb1acc863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 4.14 s (started: 2025-03-25 10:59:50 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "res_np = np.matmul(array1, array2)"
      ],
      "metadata": {
        "id": "sIm5FCbupd2R",
        "ExecuteTime": {
          "end_time": "2025-03-25T09:00:34.956250Z",
          "start_time": "2025-03-25T09:00:34.276150Z"
        },
        "outputId": "01e85adf-5aff-4ac2-c37a-f4261c9f7587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 656 ms (started: 2025-03-25 11:00:34 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "res_tensor = torch.matmul(tensor1, tensor2)"
      ],
      "metadata": {
        "id": "I547_wTAqgwB",
        "ExecuteTime": {
          "end_time": "2025-03-25T09:00:39.511834Z",
          "start_time": "2025-03-25T09:00:39.506291Z"
        },
        "outputId": "c4c747c9-98de-4a7a-a203-3ab0f052f04c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0 ns (started: 2025-03-25 11:00:39 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to extract numeric values from the tensor and use them as Python numbers, we have several options:\n",
        "- index the tensor and then call [`torch.Tensor.item()`](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html) on a single value;\n",
        "- convert the tensor to a list [`torch.Tensor.tolist()`](https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html#torch.Tensor.tolist) and treat it as a Python list;\n",
        "- convert the tensor to a np array [`torch.Tensor.numpy()`]() and treat it as a np array - this step needs the tensor to be transfered back on CPU before-hand with: `torch.Tensor.cpu()`."
      ],
      "metadata": {
        "id": "BMtEj9lj-2M3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = res_tensor.cpu().numpy()\n",
        "print(x[0][0])"
      ],
      "metadata": {
        "id": "oVfJ4MdA-428",
        "ExecuteTime": {
          "end_time": "2025-03-25T09:03:17.443287Z",
          "start_time": "2025-03-25T09:03:17.188814Z"
        },
        "outputId": "ffde96c8-6417-48df-bb76-7cdb03fc9719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[25.388868 25.574104 25.140581 ... 24.275602 24.590689 23.202984]\n",
            " [21.630035 22.82324  22.781076 ... 21.470478 21.785397 20.421955]\n",
            " [25.937572 26.113611 25.7182   ... 26.919748 25.543655 24.606337]\n",
            " ...\n",
            " [26.997513 26.275156 25.611094 ... 26.218752 25.048264 23.793365]\n",
            " [23.705494 26.177086 23.733242 ... 25.277853 23.60301  22.417557]\n",
            " [28.48238  27.043062 26.20988  ... 25.166992 25.430672 24.647043]]\n",
            "time: 250 ms (started: 2025-03-25 11:03:17 +02:00)\n"
          ]
        }
      ],
      "execution_count": null
    }
  ]
}